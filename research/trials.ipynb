{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d31321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/codespaces-blank/Source-Code-Analysis-Generative-AI/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘test_repo’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path=\"repo_code/\"\n",
    "\n",
    "repo=Repo.clone_from(\"https://github.com/Harshitnitw/End-to-End-Medical-Chatbot-Generative-AI\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f4e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=GenericLoader.from_filesystem(repo_path,\n",
    "                                     glob=\"**/*\",\n",
    "                                     suffixes=[\".py\"],\n",
    "                                     parser=LanguageParser(language=Language.PYTHON, parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo/setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Harshit Kedia',\\n    author_email= 'harshitkedia321@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_splitter=RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                               chunk_size=500,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=document_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a6dcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo/setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Harshit Kedia',\\n    author_email= 'harshitkedia321@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'test_repo/store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_mistralai import MistralAIEmbeddings\\nfrom langchain_pinecone import PineconeVectorStore  \\n\\nimport os\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\nos.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\\n\\nembeddings = MistralAIEmbeddings(\\n    model=\"mistral-embed\",\\n)\\n\\nindex_name=\"medical\"'),\n",
       " Document(metadata={'source': 'test_repo/store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='if pc.has_index(index_name): \\n    index = pc.Index(index_name)      \\n    docsearch = PineconeVectorStore(  \\n        index=index, embedding=embeddings  \\n    )\\n    print(\"vectorstore already present with the following stats: \", index.describe_index_stats())\\nelse:\\n    extracted_data=load_pdf_file(data=\"Data/\")\\n    new_extracted_data=extracted_data[0:10]\\n    text_chunks=text_split(new_extracted_data)'),\n",
       " Document(metadata={'source': 'test_repo/store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='index = pc.Index(index_name)      \\n    pc.create_index(\\n        name=index,\\n        dimension=1024, # Replace with your model dimensions\\n        metric=\"cosine\", # Replace with your model metric\\n        spec=ServerlessSpec(\\n            cloud=\"aws\",\\n            region=\"us-east-1\"\\n        ) \\n    )\\n    docsearch=PineconeVectorStore.from_documents(\\n        documents=text_chunks,\\n        index_name=index_name,\\n        embedding=embeddings\\n    )'),\n",
       " Document(metadata={'source': 'test_repo/store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content=')\\n    print(\"New vectorstore created with the following stats: \", index.describe_index_stats())'),\n",
       " Document(metadata={'source': 'test_repo/app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_mistralai import MistralAIEmbeddings, ChatMistralAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom src.prompt import *'),\n",
       " Document(metadata={'source': 'test_repo/app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import make_request\\nfrom dotenv import load_dotenv\\nimport os\\nload_dotenv()'),\n",
       " Document(metadata={'source': 'test_repo/app.py', 'language': <Language.PYTHON: 'python'>}, page_content='app=Flask(__name__)\\n\\nos.environ[\"PINECONE_API_KEY\"]=os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\"MISTRAL_API_KEY\"]=os.environ.get(\"MISTRAL_API_KEY\")\\n\\nembeddings = MistralAIEmbeddings(\\n    model=\"mistral-embed\",\\n)\\n\\nindex_name=\"medical\"\\n\\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\\nindex = pc.Index(index_name)      \\ndocsearch = PineconeVectorStore(  \\n    index=index, embedding=embeddings  \\n)\\n\\nretriever=docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})'),\n",
       " Document(metadata={'source': 'test_repo/app.py', 'language': <Language.PYTHON: 'python'>}, page_content='llm = ChatMistralAI(\\n    model=\"mistral-small-latest\",\\n    temperature=0.6,\\n    max_retries=3,\\n    max_tokens=500\\n)\\n\\nprompt=ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\",system_prompt),\\n        (\"human\",\"{input}\")\\n    ]\\n)\\n\\nquestion_answer_chain=create_stuff_documents_chain(llm, prompt)\\nrag_chain=create_retrieval_chain(retriever,question_answer_chain)\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo/app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\"/get\", methods=[\"GET\",\"POST\"])\\ndef chat():\\n    input=request.form[\"msg\"]\\n    # response=rag_chain.invoke({\"input\":input})\\n    response=make_request(rag_chain=rag_chain,question=input)\\n    return str(response[\"answer\"])\\n\\nif __name__==\\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo/template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo/template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo/src/helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport time\\n\\ndef load_pdf_file(data):\\n    loader=DirectoryLoader(data,\\n                           glob=\"*.pdf\",\\n                           loader_cls=PyPDFLoader)    \\n    documents=loader.load()\\n    return documents'),\n",
       " Document(metadata={'source': 'test_repo/src/helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks'),\n",
       " Document(metadata={'source': 'test_repo/src/helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def make_request(rag_chain,question, delay=1):\\n    while(True):\\n        try:\\n            answer = rag_chain.invoke({\"input\":question})'),\n",
       " Document(metadata={'source': 'test_repo/src/helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Check if the response has a status_code attribute (unlikely for LangChain outputs)\\n            if hasattr(answer, \\'status_code\\') and answer.status_code == 429:\\n    #             # print(f\"Rate limit hit! Retrying in {delay} seconds...\")\\n                time.sleep(delay)\\n                # delay *= 2  # Exponential backoff\\n                continue  # Retry the request'),\n",
       " Document(metadata={'source': 'test_repo/src/helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# print(\"Answer: \", answer)\\n            # print(\"----------\")\\n            # with open(\"answers.txt\", \"a\") as f:\\n            #     f.write(f\"Question: {question}\\\\n\")\\n            #     f.write(f\"Answer: {answer}\\\\n\")\\n            #     f.write(\"----------\\\\n\")\\n            return answer  # Return the answer if successful\\n\\n        except Exception as e:\\n    #         print(f\"Error occurred: {e}\")\\n            time.sleep(delay)\\n            # delay *= 2  # Increase delay for next retry'),\n",
       " Document(metadata={'source': 'test_repo/src/helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# print(\"Max retries reached. Skipping this question.\")\\n    # return None'),\n",
       " Document(metadata={'source': 'test_repo/src/prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt=(\\n    \"\"\"\\n    Use the following pieces of retrieved content to assist in question answering task. If you don\\'t know the answer, say that you don\\'t know. Answer concisely.\\n    \\\\n\\\\n\\n    {context}\\n    \"\"\"\\n)')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae42f899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3856b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/codespaces-blank/Source-Code-Analysis-Generative-AI/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspaces/codespaces-blank/Source-Code-Analysis-Generative-AI/venv/lib/python3.10/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=Chroma.from_documents(texts,embedding=embeddings,persist_directory='database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2082/3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c17673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22959/3310734444.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory=ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f112037",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478675e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is make_request function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af4ced15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `make_request` function is defined to repeatedly invoke a retrieval-augmented generation (RAG) chain with a given question until it succeeds. Here is a breakdown of the function:\n",
      "\n",
      "```python\n",
      "def make_request(rag_chain, question, delay=1):\n",
      "    while True:\n",
      "        try:\n",
      "            answer = rag_chain.invoke({\"input\": question})\n",
      "            # Assuming the function returns the answer or processes it further\n",
      "            return answer\n",
      "        except Exception as e:\n",
      "            # Handle the exception (e.g., log it, retry after a delay)\n",
      "            logging.error(f\"Error occurred: {e}\")\n",
      "            time.sleep(delay)\n",
      "```\n",
      "\n",
      "- **Parameters:**\n",
      "  - `rag_chain`: The RAG chain object that will be used to process the question.\n",
      "  - `question`: The question to be asked.\n",
      "  - `delay`: The delay in seconds before retrying the request after an exception (default is 1 second).\n",
      "\n",
      "- **Functionality:**\n",
      "  - The function enters an infinite loop (`while True:`).\n",
      "  - Inside the loop, it attempts to invoke the `rag_chain` with the given question.\n",
      "  - If the invocation is successful, it returns the answer.\n",
      "  - If an exception occurs, it logs the error, waits for the specified delay, and then retries the request.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dc393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972ba07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
